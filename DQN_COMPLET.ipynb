{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DQN COMPLET.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDbLFhIdkUG6"
      },
      "source": [
        "# **Import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvq9DCTvkUG_",
        "outputId": "6f9364f2-2cc5-448c-c75b-c5d49dd32aa8"
      },
      "source": [
        "import os\n",
        "import h5py\n",
        "import turtle\n",
        "import random\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from itertools import count\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from math import cos, sin, radians , exp, sqrt\n",
        "print(\"GPUs : \", tf.config.list_physical_devices('GPU'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPUs :  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xdt0HqakUHA"
      },
      "source": [
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG7VSWXxkUHA"
      },
      "source": [
        "#**Ce qu'il me reste à faire :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWqMrh4qkUHB"
      },
      "source": [
        "*   **WIP :** Changer le support Render() Turtle vers autre librairie\n",
        "\n",
        "*   **WIP :** optimiser le programme( surtout l'environnement) et fusionner les classes pouvant l'être\n",
        "\n",
        "*   **WIP :** Utiliser le GPU voir TPU si possible\n",
        "\n",
        "*   **WIP :** débloquer l'implementation des obstacles\n",
        "\n",
        "*   **WIP :** généraliser le programme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqhUJ46z7aWp"
      },
      "source": [
        "# **Paramètres**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGGhmBsfJc-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c7f1f2-5a15-4c8c-cdde-5ffc2004b800"
      },
      "source": [
        "class para:\n",
        "    \"\"\"\n",
        "    Classe de tous les paramètres modifiables\n",
        "    \"\"\"\n",
        "        # hyperparamètres de l'agent :      \n",
        "    episodes = 700                      # nb d'épisode d'entrainement\n",
        "\n",
        "    dueling = True                      # True si mise en place de double dueling networks\n",
        "    tau_update=True\n",
        "    dir=\"model\"\n",
        "    save = False\n",
        "    modeload = False\n",
        "\n",
        "        # données de compilation\n",
        "    LearningRate = 1e-3\n",
        "    opti=tf.keras.optimizers.RMSprop(learning_rate=LearningRate, rho=0.95, epsilon=0.01)    # meilleur pour l'instant c RMSPROP [RMSprop(lr=self.learning_rate, rho=0.95, epsilon=0.01]\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy()                                          # meilleur pour l'instant c categorical crossentropy\n",
        "    metrique=[tf.keras.metrics.CategoricalCrossentropy()]                                   # meilleur pour l'instant c categorical crossentropy\n",
        "    initializer= tf.keras.initializers.Ones()\n",
        "\n",
        "        # exploration ou exploitation\n",
        "    eps_st = 1\n",
        "    eps_end = 0.01\n",
        "    eps_decay = 0.001\n",
        "\n",
        "        #gestion du target+Memoire\n",
        "    target_update = 16                  #nombre de tour entre chaque actualisation du target net\n",
        "    memory_size = 20000                  #taille de la Replay memory\n",
        "    batch_size = 64                     #taille du batch de Replay memory sur lequel j'entraine le modèle à chaque tour\n",
        "    batchsize = 32                      # taille du batch fit\n",
        "    tau=0.1\n",
        "    PER_epsi = 0.01\n",
        "    PER_alpha = 0.6\n",
        "\n",
        "        #variables d'environnement : \n",
        "    renderfull=False                    # True fait en sorte que je puisse voir en temps réel le chemin\n",
        "    render = False\n",
        "    gamma = 0.999                       # taux de diminution de la récompense\n",
        "\n",
        "    A = [0,0]\n",
        "    B = [30,0]\n",
        "    size= [(abs(B[0])+abs(B[1]))*2+2,(abs(B[0])+abs(B[1]))*2+2,2]\n",
        "    astar = (2**(6*int(np.log10(sqrt(B[0]**2 + B[1]**2)) )))-1\n",
        "    print(astar)\n",
        "    print(size)\n",
        "\n",
        "    def_step = 400                      # le nombre maximal de pas authorisé par tour\n",
        "        \n",
        "    obstacles = [[20,-56],[120,12]]\n",
        "    obstacles_size = [50,30]\n",
        "\n",
        "        \n",
        "        #donnes de l'agent\n",
        "    limit = (B[0]**2 + B[1]**2)*4       # la limite de distance au carré \n",
        "    actionspace = 4                     # me donne le nombre de déplacements possibles ( 4 = 90 °) nb/360 = nb de degré de Delta\n",
        "    j = 1                               # la taille des pas que je fais\n",
        "\n",
        "        #données d'apprentissage/ de visulisation des données\n",
        "\n",
        "    nb_objets=1                         # le nombre d'objets différents (turtle,obstacle,pointB,A,etc)\n",
        "    M = 5                               # j'aurai la moyenne des M derniers nombres mouvements sur le plot\n",
        "    M2 = 20                             # j'aurai la proportion des M2 dernières fins réelles sur le plot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63\n",
            "[102, 102, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlL0zMo756Bx"
      },
      "source": [
        "# **Matplot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkGOTMQHJ3hQ"
      },
      "source": [
        "def plot (values,period,what):\n",
        "    \"\"\"\n",
        "    sert à afficher les données de façon clean\n",
        "    \"\"\"\n",
        "    if what == 0 : \n",
        "        if is_ipython: display.clear_output(wait=True)\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    if what ==0:\n",
        "        plt.title(\"Training\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Duration\")\n",
        "    elif what==1:\n",
        "        plt.title(\"Evaluation des fins réelles\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"statut de la fin\")\n",
        "    plt.plot(values)\n",
        "\n",
        "    moving_avg = get_moving_average(values,period)\n",
        "    \n",
        "    plt.plot(moving_avg)\n",
        "    plt.pause(0.001)\n",
        "    return moving_avg\n",
        "\n",
        "def get_moving_average(valeurs,period):\n",
        "    if len(valeurs)>= period:\n",
        "        return np.insert(pd.Series(valeurs).rolling(window=period).mean().iloc[period-1:].values,0,[0]*(period-1))\n",
        "    else:\n",
        "        return np.zeros(len(valeurs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isalDx5N6vN8"
      },
      "source": [
        "# **Classe DQN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyO_1VkHJJiK"
      },
      "source": [
        "class DQN(tf.Module):  \n",
        "    \"\"\"\n",
        "    création d'un réseau de neurone profond \n",
        "    \"\"\"\n",
        "    def __init__(self,inputnb,actionspace,ler,named,dir,opti,lossing,metrique,dueling,tau,tau_update,initializer):\n",
        "        self.tau_update=tau_update\n",
        "        self.dueling=dueling\n",
        "        self.TAU = tau\n",
        "        if not dueling:\n",
        "            self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2D(filters=3,kernel_size=2, strides=1, activation='relu',input_shape=(inputnb[0],inputnb[1],inputnb[2])),\n",
        "            tf.keras.layers.MaxPool2D(pool_size=2, strides=1, padding='valid'),\n",
        "            tf.keras.layers.PRelu(),\n",
        "            tf.keras.layers.Conv2D(filters=2,kernel_size=2, strides=2, activation='relu'),\n",
        "            tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'),\n",
        "            tf.keras.layers.PReLU(),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(1024,activation=\"elu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(512, activation=\"elu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(256, activation='elu', kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(128, activation='elu', kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(64,  activation='elu', kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(32,  activation='elu', kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(actionspace, activation='linear', kernel_initializer=initializer)])\n",
        "\n",
        "            self.model.compile(optimizer=opti, loss=lossing, metrics=metrique)\n",
        "            self.model.summary()\n",
        "        else:\n",
        "            input=tf.keras.layers.Input(shape=inputnb,name=\"Input\")\n",
        "\n",
        "            convo = tf.keras.Sequential([\n",
        "                tf.keras.layers.Conv2D(filters=3,kernel_size=2, strides=1, activation=\"relu\",input_shape=(inputnb[0],inputnb[1],inputnb[2])),\n",
        "                tf.keras.layers.MaxPool2D(pool_size=2, strides=1, padding=\"valid\"),\n",
        "                tf.keras.layers.Conv2D(filters=2,kernel_size=2, strides=2, activation=\"relu\"),\n",
        "                tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding=\"valid\"),\n",
        "                tf.keras.layers.PReLU(),\n",
        "                tf.keras.layers.Flatten(),\n",
        "                tf.keras.layers.Dense(1024,activation=\"relu\", kernel_initializer=initializer)],name=\"Convolution\")(input)\n",
        "\n",
        "            value_net = tf.keras.Sequential([\n",
        "                tf.keras.layers.Dense(128,activation=\"relu\", kernel_initializer=initializer),\n",
        "                tf.keras.layers.Dense(1, kernel_initializer=initializer),\n",
        "                tf.keras.layers.Lambda(lambda s: tf.keras.backend.expand_dims(s[:, 0], -1),output_shape=(actionspace,))],name=\"Value_network\")(convo)\n",
        "\n",
        "            advantage_net = tf.keras.Sequential([ \n",
        "                tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer=initializer),\n",
        "                tf.keras.layers.Dense(actionspace, kernel_initializer=initializer),\n",
        "                tf.keras.layers.Lambda(lambda a: a[:, :] - tf.keras.backend.mean(a[:, :], keepdims=True),output_shape=(actionspace,))],name=\"Advantage_network\")(convo)\n",
        "\n",
        "            output = tf.keras.layers.Add()([value_net, advantage_net])\n",
        "\n",
        "            \"\"\"distance_net = tf.keras.Sequential([\n",
        "                tf.keras.layers.Dense(128),\n",
        "                tf.keras.layers.Flatten(),\n",
        "                tf.keras.layers.Dense(actionspace)])(input)\n",
        "            net = tf.keras.layers.Concatenate()([Resnet,distance_net])\"\"\"\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "            self.model = tf.keras.Model(input, output,name=named)\n",
        "            self.model.compile(optimizer=opti, loss=lossing, metrics=metrique)\n",
        "            tf.keras.utils.plot_model(self.model, to_file='réseau.png')\n",
        "            self.model.summary()\n",
        "\n",
        "        if not os.path.exists(dir): os.makedirs(dir)\n",
        "        self.saveplace = os.path.join(dir,named+\".h5\") \n",
        "\n",
        "    def tautransfer(self,policy_model):\n",
        "        if not self.tau_update:\n",
        "            self.transfer(policy_model)\n",
        "        else:\n",
        "            q_model_theta = policy_model.get_weights()\n",
        "            target_model_theta = self.model.get_weights()\n",
        "            i = 0\n",
        "            for q_weight, target_weight in zip(q_model_theta, target_model_theta):\n",
        "                target_weight = target_weight * (1-self.TAU) + q_weight * self.TAU\n",
        "                target_model_theta[i] = target_weight\n",
        "                i += 1\n",
        "            self.model.set_weights(target_model_theta)\n",
        "\n",
        "    def transfer(self,model):\n",
        "        self.model.set_weights(model.get_weights())\n",
        "\n",
        "    def fitting(self,target_net,gammaparam,batchsize,memory,batch_size):\n",
        "        experiences = memory.sample(batchsize)\n",
        "        states, actions, rewards , next_states, dones = extract_tensors(list(zip(*experiences))[1])\n",
        "\n",
        "        target = tf.squeeze(self.model.predict(states)).numpy()\n",
        "        target_next = tf.squeeze(self.model.predict(next_states)).numpy()\n",
        "        target_val = tf.squeeze(target_net.predict(next_states)).numpy()\n",
        "\n",
        "        erreur = np.zeros(batchsize)\n",
        "        for i in range(batchsize):\n",
        "            val = target[i][actions[i]]\n",
        "            if dones[i]: target[i][actions[i]] = rewards[i]\n",
        "            else:target[i][actions[i]] = gammaparam * target_val[i][np.argmax(target_next[i])] + rewards[i]\n",
        "            erreur[i] = abs(val - target[i][actions[i]])\n",
        "        \n",
        "        for i in range(param.batch_size):\n",
        "            idx = experiences[i][0]\n",
        "            memory.update(idx, erreur[i])\n",
        "    \n",
        "        self.model.fit(states, tf.constant(target), batch_size=batch_size,sample_weight=None,verbose=0, epochs=3)\n",
        "        \n",
        "    \n",
        "    def load(self):\n",
        "        print(\"------------CHARGEMENT MODELE------------\")\n",
        "        try:\n",
        "            self.model = tf.keras.models.load_model(self.saveplace)\n",
        "            print(\"--------------MODELE CHARGE--------------\")\n",
        "        except Exception as ex: print(\"échec du chargement : \",ex)\n",
        "\n",
        "    def save(self):\n",
        "        print(\"----------ENREGISTREMENT MODELE----------\")\n",
        "        try: \n",
        "            self.model.save(self.saveplace) \n",
        "            print(\"------------MODELE ENREGISTRE------------\")\n",
        "        except Exception as ex: print(\"échec de l'enregistrement : \",ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1E3gfzp6oI4"
      },
      "source": [
        "# **Experience et arbre binaire**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5Pl5aKuJUft"
      },
      "source": [
        "Experience = namedtuple(\"Experience\",(\"state\",\"action\",\"reward\",\"next_state\",\"done\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht3_ngBYJUkY"
      },
      "source": [
        "class SumTree:\n",
        "    write = 0\n",
        "    \"\"\"\n",
        "    crée un arbre binaire (bibliothèque venant de Github)\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2*capacity - 1)\n",
        "        self.data = np.zeros(capacity, dtype=object)\n",
        "\n",
        "    def _propagate(self, idx, change):\n",
        "        parent = (idx - 1)//2\n",
        "        self.tree[parent] += change\n",
        "        if parent != 0: self._propagate(parent, change)\n",
        "\n",
        "    def _retrieve(self, idx, s):\n",
        "        left = 2 * idx + 1\n",
        "        right = left + 1\n",
        "        if left >= len(self.tree): return idx\n",
        "        if s <= self.tree[left]: return self._retrieve(left, s)\n",
        "        else: return self._retrieve(right, s-self.tree[left])\n",
        "\n",
        "    def add(self, p, data):\n",
        "        idx = self.write + self.capacity - 1\n",
        "        self.data[self.write] = data\n",
        "        self.update(idx, p)\n",
        "        self.write += 1\n",
        "        if self.write >= self.capacity: self.write = 0\n",
        "\n",
        "    def update(self, idx, p):\n",
        "        change = p - self.tree[idx]\n",
        "        self.tree[idx] = p\n",
        "        self._propagate(idx, change)\n",
        "\n",
        "    def get(self, s):\n",
        "        idx = self._retrieve(0, s)\n",
        "        dataIdx = idx - self.capacity + 1\n",
        "        return (idx, self.tree[idx], self.data[dataIdx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGx-VR5HJUkY"
      },
      "source": [
        "# **Mémoire**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRYb6mEnJUkZ"
      },
      "source": [
        "class Replay_memory():\n",
        "    \"\"\"\n",
        "    Je gère la mémoire \n",
        "    \"\"\"\n",
        "    def __init__(self,capa,a,e):\n",
        "        self.tree = SumTree(capa)\n",
        "        self.e = e\n",
        "        self.a = a\n",
        "\n",
        "    def add(self, experience, policynet, targetnet,gamma):\n",
        "        state, action, reward , next_state, done = extract_tensors([experience])\n",
        "\n",
        "        target = policynet(state).numpy()\n",
        "        target_next = policynet(next_state).numpy()\n",
        "        target_val = targetnet(next_state).numpy()\n",
        "\n",
        "        val = target[0][action[0]]\n",
        "        if done[0]: target[0][action[0]] = reward[0]\n",
        "        else:target[0][action[0]] = gamma * target_val[0][np.argmax(target_next[0])] + reward[0]\n",
        "        erreur = abs(val - target[0][action[0]])\n",
        "        self.tree.add((erreur + self.e) ** self.a, experience)\n",
        "\n",
        "    def sample(self, n):\n",
        "        batch = []\n",
        "        moyenne = self.tree.tree[0] / n\n",
        "        for i in range(n):\n",
        "            a = moyenne * i\n",
        "            b = moyenne * (i + 1)\n",
        "            idx, p, data = self.tree.get(random.uniform(a, b))\n",
        "            batch.append([idx, data])\n",
        "        return batch\n",
        "\n",
        "    def update(self, idx, error):\n",
        "        self.tree.update(idx, (error + self.e) ** self.a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOCcPiMz6ZTU"
      },
      "source": [
        "# **Politique EpsilonGreedy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvyfJ4QIJaJ0"
      },
      "source": [
        "class Greedy():\n",
        "    \"\"\"\n",
        "    Me donne la politique en matière d'exploration/ Exploitation ( via le facteur *Epsilon)\n",
        "    \"\"\"\n",
        "    def __init__(self,start,end,decay):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.decay = decay\n",
        "\n",
        "    def get_explo_rate(self, current_step):\n",
        "        return self.end + exp(-1. * self.decay * current_step) * (self.start - self.end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hgV0H646WGB"
      },
      "source": [
        "# **Agent décisionnel**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwOyhx6iJbt3"
      },
      "source": [
        "class Agent():\n",
        "    \"\"\"\n",
        "    Agent décisionnel (me donne l'action à faire) (MDP)\n",
        "    \"\"\"\n",
        "    def __init__(self,strategy,num_actions):\n",
        "        self.num_actions = num_actions\n",
        "        self.current_step = 0\n",
        "        self.strategy = strategy\n",
        "    \n",
        "    def select_action(self,state,policy_net):\n",
        "        rate = self.strategy.get_explo_rate(self.current_step)\n",
        "        self.current_step +=1\n",
        "\n",
        "        if rate > random.random():\n",
        "            return random.randrange(self.num_actions)\n",
        "        else:\n",
        "            return ((tf.squeeze(tf.math.argmax(policy_net.model(tf.expand_dims(state,0)),axis=1))).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ognkpCPT6Q93"
      },
      "source": [
        "# **Classe Environnement**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_WK_fsSkUHH"
      },
      "source": [
        "class ENV:   \n",
        "    \"\"\"\n",
        "    l'environnement, mon agent, ses caractéristiques\n",
        "    \"\"\"\n",
        "    def __init__(self,A,B,j,Max_steps,obsta,size,actionspace,limit,initset,nb_objet):\n",
        "        self.actions_space = actionspace\n",
        "        self.posA = A\n",
        "        self.posB = B\n",
        "        self.state = [A[0],A[1]]\n",
        "        self.j = j\n",
        "        self.path = [A]\n",
        "        self.etape = 0\n",
        "        self.stepmax = Max_steps\n",
        "        self.obstacles = obsta\n",
        "        self.nb_objets = nb_objet\n",
        "        self.size = size\n",
        "        self.initset = initset\n",
        "        self.perte = -100/(size[0]*size[1])\n",
        "        self.done = False\n",
        "\n",
        "        if initset:\n",
        "            screen = turtle.Screen()\n",
        "            screen.title(\"Chemin le plus court\")\n",
        "            self.bob= turtle.Turtle()\n",
        "            self.bob.pos = self.posA\n",
        "            self.bob.speed(0)\n",
        "            self.bob.pu()\n",
        "            self.bob.goto(self.posA[0], self.posA[1])\n",
        "            self.bob.pd()\n",
        "            self.Acr(self.posA[0],self.posA[1])\n",
        "            self.Bcr(self.posB[0],self.posB[1])\n",
        "            self.obstacle_Spawn()\n",
        "\n",
        "    def Acr(xa,ya):\n",
        "        A = turtle.Turtle()\n",
        "        A.speed(0)\n",
        "        A.pu()\n",
        "        A.color(\"red\")\n",
        "        A.shape(\"square\")\n",
        "        A.shapesize(stretch_wid=1,stretch_len=1)\n",
        "        A.goto(xa,ya)\n",
        "\n",
        "    def Bcr(xb,yb):\n",
        "        B = turtle.Turtle()\n",
        "        B.pu()\n",
        "        B.speed(0)\n",
        "        B.color(\"blue\")\n",
        "        B.shape(\"square\")\n",
        "        B.shapesize(stretch_wid=1,stretch_len=1)\n",
        "        B.goto(xb, yb)\n",
        "\n",
        "    def obstacle_Spawn(self):\n",
        "        for w in range (0,len(self.obstacles)):\n",
        "            obstacle = turtle.Turtle()\n",
        "            obstacle.speed(0)\n",
        "            obstacle.pu()\n",
        "            obstacle.shape(\"square\")\n",
        "            obstacle.color(\"brown\")\n",
        "            obstacle.shapesize(stretch_wid=5, stretch_len=3)\n",
        "            obstacle.goto(self.obstacles[w][0],self.obstacles[w][1])\n",
        "\n",
        "    def Astar(self):\n",
        "        x=[]\n",
        "        for i in range(self.actions_space): x.append(((self.state[0]+self.j*cos(radians((360/self.actions_space)*i))-self.posB[0])**2+(self.state[1]+self.j*sin(radians((360/self.actions_space)*i))-self.posB[1])**2))\n",
        "        return np.argmin(x)\n",
        "\n",
        "    def render(self): # n'est utliser que si initset était true\n",
        "        if self.initset == True:\n",
        "            self.bob.goto(self.state[0],self.state[1])\n",
        "        else:\n",
        "            print (\"impossible d'afficher une donnée\")\n",
        "\n",
        "    def renderfull(self):  # je render tout le chemin d'une turtle\n",
        "        print(self.path)\n",
        "        screen = turtle.Screen()\n",
        "        screen.title(\"Chemin le plus court\")\n",
        "        self.bob= turtle.Turtle()\n",
        "        self.bob.pos = self.posA\n",
        "        self.bob.speed(1)\n",
        "        self.bob.pu()\n",
        "        self.bob.goto(self.posA[0], self.posA[1])\n",
        "        self.bob.pd()\n",
        "        self.Acr(self.posA[0],self.posA[1])\n",
        "        self.Bcr(self.posB[0],self.posB[1])\n",
        "        self.obstacle_Spawn()\n",
        "        self.bob.goto(self.path[-1])\n",
        "        self.bob.pencolor(\"red\")\n",
        "        for i in range (1,len(self.path)):            # je repasse en allant seulement sur les cases validées \n",
        "            self.bob.goto(self.path[-i])\n",
        "        turtle.done()\n",
        "\n",
        "    def get_state(self):\n",
        "        stat = np.zeros(self.size)\n",
        "        stat[self.state[0]][self.state[1]][0] = 1\n",
        "        stat[self.posB[0]][self.posB[1]][1] = 10\n",
        "        return tf.constant(stat,dtype=float)\n",
        "\n",
        "    def take_action(self,action):\n",
        "        reward= self.step(action)\n",
        "        return (reward,self.done)\n",
        "\n",
        "    def reset(self):\n",
        "        self.etape = 0\n",
        "        self.path = [[self.posA[0],self.posA[1]]]\n",
        "        self.state = [self.posA[0],self.posA[1]]\n",
        "        return (self.posA)\n",
        "\n",
        "    def close():\n",
        "        turtle.bye()\n",
        "\n",
        "    def step(self,action):\n",
        "        self.state[0]+= int(self.j*cos(radians((360/self.actions_space)*action)))\n",
        "        self.state[1]+= int(self.j*sin(radians((360/self.actions_space)*action)))\n",
        "\n",
        "        self.distance = (self.state[0]-self.posB[0] )**2  + ( self.state[1]-self.posB[1] )**2\n",
        "        ''' # Implementation de la gestion d'obstacles\n",
        "        for i in range(0,len(self.obstacles)):\n",
        "            if( (    abs(self.state[0] - self.obstacles[i][0]) <=30 ) and (  abs( self.state[1] - obstacles[i][1]) <=50  )):\n",
        "                reward = -50\n",
        "                break \n",
        "        '''\n",
        "        if (self.distance <= self.j**2):\n",
        "            self.state = self.posB\n",
        "            reward = 1000\n",
        "            self.etape = self.stepmax\n",
        "            self.path.append(self.posB)\n",
        "        else:\n",
        "            reward = self.perte\n",
        "            self.path.append(self.state)\n",
        "\n",
        "        self.etape +=1     # j'ajoute 1 à l'étape puis vérifie si le nombre maximal d'étape pas simulation est atteint ou lorsque l'objet atteint B \n",
        "\n",
        "        if (self.etape >= self.stepmax ):self.done = True\n",
        "        else: self.done = False\n",
        "\n",
        "        if self.state[0]>=self.size[0]-3 or self.state[1]>=self.size[1]-3 or self.state[0]<=-(self.size[0]-3) or self.state[1]<=-(self.size[1]-3):\n",
        "            reward = -10000\n",
        "\n",
        "        if self.state[0]>=self.size[0]-1 or self.state[1]>=self.size[1]-1 or self.state[0]<=-(self.size[0]-1) or self.state[1]<=-(self.size[1]-1):\n",
        "            self.done = True\n",
        "            reward = -10000\n",
        "\n",
        "        return reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Jp3iLG6BBe"
      },
      "source": [
        "# **extraction tenseur**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VFkM535J1aw"
      },
      "source": [
        "def extract_tensors(experiences):\n",
        "    \"\"\"\n",
        "    J'extrais 5 tenseurs à partir d'un named tuples experiences \n",
        "    \"\"\"\n",
        "    batch = Experience(*zip(*experiences))\n",
        "    t1 = tf.concat([batch.state],-1)\n",
        "    t2 = np.concatenate([batch.action],-1)\n",
        "    t3 = np.concatenate([batch.reward],-1)\n",
        "    t4 = tf.concat([batch.next_state],-1)\n",
        "    t5 = np.concatenate([batch.done],-1)\n",
        "    return(t1,t2,t3,t4,t5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3Ig_NZjJUkb"
      },
      "source": [
        "# **Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "FFkeQBY0J6CW",
        "outputId": "facb8448-2b14-4a7c-9236-ebafb804d6a9"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    param = para()\n",
        "    end = 0\n",
        "    endingpoint=[]\n",
        "    policy_net = DQN(param.size,param.actionspace,param.LearningRate,\"Policy\",param.dir,param.opti,param.loss,param.metrique,param.dueling,param.tau,param.tau_update,param.initializer)\n",
        "    target_net = DQN(param.size,param.actionspace,param.LearningRate,\"Target\",param.dir,param.opti,param.loss,param.metrique,param.dueling,param.tau,param.tau_update,param.initializer)\n",
        "    if param.modeload:policy_net.load()\n",
        "    target_net.transfer(policy_net.model)\n",
        "\n",
        "    env = ENV(param.A,param.B,param.j,param.def_step,param.obstacles,param.size,param.actionspace,param.limit,param.renderfull,param.nb_objets)\n",
        "    greedy = Greedy(param.eps_st, param.eps_end,param.eps_decay)\n",
        "    agent = Agent(greedy,env.actions_space)\n",
        "    memory = Replay_memory(param.memory_size,param.PER_alpha,param.PER_epsi)\n",
        "\n",
        "    episode_len = []\n",
        "\n",
        "    for episode in range(param.episodes):\n",
        "        \n",
        "        env.reset()\n",
        "        state = env.get_state()\n",
        "        print(\"========================================================================================================\")\n",
        "        \n",
        "        for pas in count():\n",
        "            if param.render: env.render()\n",
        "            if episode > param.astar : action = agent.select_action(state,policy_net)\n",
        "            else:action = env.Astar()\n",
        "\n",
        "            reward,done = env.take_action(action)\n",
        "            next_state = env.get_state()\n",
        "            memory.add(Experience(state,action,reward,next_state,done),policy_net.model,target_net.model,param.gamma)\n",
        "            state = next_state\n",
        "            policy_net.fitting(target_net.model,param.gamma,param.batch_size,memory,param.batchsize)\n",
        "                \n",
        "            if pas % 25 ==0 : print(f\"pas numéro : {pas} :: state : {env.state[0]} | {env.state[1]} :: reward : {reward} :: policy = {tf.squeeze(policy_net.model(tf.expand_dims(state,0))).numpy()}\")\n",
        "\n",
        "            if env.done:\n",
        "                target_net.tautransfer(policy_net.model)\n",
        "                if (reward==1000):\n",
        "                    end+=1\n",
        "                    endingpoint.append(1)\n",
        "                else:\n",
        "                    endingpoint.append(0)\n",
        "\n",
        "                episode_len.append(pas)\n",
        "                move_avg = plot(episode_len, param.M,0)\n",
        "                print(f\"Episode : {episode+1}\\nmoyenne mobile des {param.M} épisodes : {move_avg[-1]}\\nvraie fin : {end} \\nsoit : {(end/(episode+1))*100} % \" )\n",
        "                break\n",
        "            if not param.tau_update:\n",
        "                if episode%param.target_update==0:target_net.transfer(policy_net.model)\n",
        "    \n",
        "    if param.save:policy_net.save()\n",
        "\n",
        "\n",
        "    _ = plot(endingpoint,param.M2,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Episode : 163\n",
            "moyenne mobile des 5 épisodes : 100.6\n",
            "vraie fin : 65 \n",
            "soit : 39.87730061349693 % \n",
            "========================================================================================================\n",
            "pas numéro : 0 :: state : 0 | 1 :: reward : -0.009611687812379853 :: policy = [-0.06386743  0.33974713 -0.21414529 -0.11941838]\n",
            "pas numéro : 25 :: state : 0 | 26 :: reward : -0.009611687812379853 :: policy = [-0.06386743  0.33974713 -0.21414529 -0.11941838]\n",
            "pas numéro : 50 :: state : 0 | 51 :: reward : -0.009611687812379853 :: policy = [-0.06386743  0.33974713 -0.21414529 -0.11941838]\n",
            "pas numéro : 75 :: state : 0 | 76 :: reward : -0.009611687812379853 :: policy = [-0.06386743  0.33974713 -0.21414529 -0.11941838]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-5b66ae0e9bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExperience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpas\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m25\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pas numéro : {pas} :: state : {env.state[0]} | {env.state[1]} :: reward : {reward} :: policy = {tf.squeeze(policy_net.model(tf.expand_dims(state,0))).numpy()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b354a361e32c>\u001b[0m in \u001b[0;36mfitting\u001b[0;34m(self, target_net, gammaparam, batchsize, memory, batch_size)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merreur\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1227\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    431\u001b[0m           \u001b[0mare\u001b[0m \u001b[0mprefixed\u001b[0m \u001b[0;32mwith\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mval_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \"\"\"\n\u001b[0;32m--> 433\u001b[0;31m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs, is_batch_hook)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_batch_hook\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_hooks_support_tf_logs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m    867\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       expand_composites=expand_composites)\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m   \"\"\"\n\u001b[0;32m--> 755\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m           \u001b[0;34m\"flat_sequence had %d elements.  Structure: %s, flat_sequence: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m           (len(flat_structure), len(flat_sequence), structure, flat_sequence))\n\u001b[0;32m--> 641\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msequence_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_sequence_like\u001b[0;34m(instance, args)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# ordered and plain dicts (e.g., flattening a dict but using a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# corresponding `OrderedDict` to pack it back).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0minstance_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minstance_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_sorted\u001b[0;34m(dict_)\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;34m\"\"\"Returns a sorted list of the dict keys, with error if keys not sortable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nest only supports dicts with sortable keys.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}